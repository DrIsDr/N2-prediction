{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codes for model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T07:43:34.684390Z",
     "start_time": "2020-11-22T07:43:12.396442Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # v1 fa docker gpu 0\n",
    "\n",
    "import random\n",
    "import shutil\n",
    "from functools import partial\n",
    "from glob import glob\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from pathlib import Path\n",
    "\n",
    "import cupy as cp\n",
    "import cv2\n",
    "import fastai\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import SimpleITK\n",
    "import SimpleITK as sitk\n",
    "import sklearn.metrics as skm\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from cupyx.scipy import ndimage as cu_ndimage\n",
    "from fastai.callback.mixup import MixUp\n",
    "from fastai.callback.tracker import SaveModelCallback\n",
    "from fastai.data.core import DataLoaders, Tensor, explode_types, fastuple, show_image, tensor, typedispatch\n",
    "from fastai.optimizer import SGD, Adam, QHAdam\n",
    "from fastai.torch_basics import flatten_check, is_listy, itertools, nn, set_seed, to_np, torch\n",
    "from fastai.torch_core import defaults\n",
    "from fastai.vision.all import (\n",
    "    Callback,\n",
    "    Categorize,\n",
    "    CategoryBlock,\n",
    "    ClassificationInterpretation,\n",
    "    CropPad,\n",
    "    CrossEntropyLossFlat,\n",
    "    DataBlock,\n",
    "    Datasets,\n",
    "    FileGetter,\n",
    "    GrandparentSplitter,\n",
    "    Hook,\n",
    "    Image,\n",
    "    ImageBlock,\n",
    "    ImageDataLoaders,\n",
    "    Interpretation,\n",
    "    IntToFloatTensor,\n",
    "    Learner,\n",
    "    Module,\n",
    "    MultiCategoryBlock,\n",
    "    Normalize,\n",
    "    PILBase,\n",
    "    PILImage,\n",
    "    PILImageBW,\n",
    "    Pipeline,\n",
    "    RandomCrop,\n",
    "    RandomResizedCrop,\n",
    "    RandTransform,\n",
    "    Recorder,\n",
    "    Resize,\n",
    "    ResizeMethod,\n",
    "    RocAuc,\n",
    "    RocAucBinary,\n",
    "    TensorImage,\n",
    "    TensorImageBase,\n",
    "    TfmdLists,\n",
    "    ToTensor,\n",
    "    Transform,\n",
    "    TransformBlock,\n",
    "    accuracy,\n",
    "    aug_transforms,\n",
    "    cast,\n",
    "    create_body,\n",
    "    create_head,\n",
    "    detuplify,\n",
    "    doc,\n",
    "    first,\n",
    "    get_files,\n",
    "    get_grid,\n",
    "    imagenet_stats,\n",
    "    model_meta,\n",
    "    params,\n",
    "    parent_label,\n",
    "    resnet34,\n",
    "    resnet50,\n",
    "    resnet101,\n",
    "    resnet152,\n",
    "    show_at,\n",
    "    skm_to_fastai,\n",
    "    store_attr,\n",
    "    to_image,\n",
    "    uniqueify,\n",
    "    xresnet18,\n",
    "    xresnet34,\n",
    "    xresnet50,\n",
    "    xresnet101,\n",
    "    xresnet152,\n",
    "    xse_resnet152,\n",
    "    xse_resnext18,\n",
    "    xse_resnext50,\n",
    "    xse_resnext101,\n",
    ")\n",
    "from fastcore.foundation import L\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "from termcolor import colored\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "plt.style.use(\"seaborn-deep\")\n",
    "\n",
    "plt.rc(\"font\", family=\"Times New Roman\")\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config & Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T07:43:34.723811Z",
     "start_time": "2020-11-22T07:43:34.686165Z"
    }
   },
   "outputs": [],
   "source": [
    "def random_seed(seed_value):\n",
    "    \"\"\"\n",
    "    Global random seed for CPU/GPU. \n",
    "    \"\"\"\n",
    "    assert isinstance(seed_value, int)\n",
    "\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "class Args:\n",
    "    exp_name = \"ZYF1207benchmark3-test2auc77\"\n",
    "    seed = 2\n",
    "    dpi = 200\n",
    "\n",
    "    ww = 1200\n",
    "    wl = -500\n",
    "    blacklist = [\n",
    "        \"345092\",\n",
    "        \"353437\",\n",
    "    ]\n",
    "    data_path = Path(\"/Projects/data/DJJ\")\n",
    "    # part 1 (train + validation + test0)\n",
    "    excel_f_p1 = data_path / \"part1/SPH-20191215.xlsx\"\n",
    "    ct_path_p1 = data_path / \"part1\"\n",
    "\n",
    "    # test 1\n",
    "    excel_f_t1 = data_path / \"test1/test1_new.xlsx\"\n",
    "    ct_path_t1 = data_path / \"test1\"\n",
    "\n",
    "    # test 2\n",
    "    excel_f_t2 = data_path / \"test2/test2_new.xlsx\"\n",
    "    ct_path_t2 = data_path / \"test2\"\n",
    "\n",
    "    output_path = data_path / \"processed_ct_r75\"\n",
    "    spacing = [0.6, 0.6, 0.6]\n",
    "\n",
    "    fixed_half_edge = 75  # half the length of nodule 3D-Patch\n",
    "    use_existing = True\n",
    "    split_p = (0.8, 0.9)  # Split points for training, validation, and test sets\n",
    "    shuffle_dataset = False\n",
    "\n",
    "    rotation = 180\n",
    "    r_pos = 1.1 # ratio of class1 : class0 during training\n",
    "    radius_middle = 66  # half the length of crop\n",
    "    resize_size = 112  # size of network's input\n",
    "    n_in = 3  # number of input channel\n",
    "\n",
    "    batch_size = 64\n",
    "\n",
    "    num_workers = 0  # Number of CPU threads in DataLoader\n",
    "    ps = 0.5\n",
    "    lr = 1e-3\n",
    "    wd = 1e-3\n",
    "    mom = 0.9\n",
    "    epoch = 150\n",
    "    freeze_epochs = 70\n",
    "    pct_start = 0.3\n",
    "    loss_weight = [1.0, 1.1]\n",
    "\n",
    "    test_code = True\n",
    "    pre_processing = False  # whether need to pre-process raw CT data\n",
    "    plot_validate_result = True\n",
    "\n",
    "    vmin = -0.025\n",
    "    vmax = 0.15\n",
    "\n",
    "\n",
    "args = Args()\n",
    "random_seed(args.seed)\n",
    "\n",
    "defaults.callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages' Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T07:43:34.753638Z",
     "start_time": "2020-11-22T07:43:34.725546Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"fastai: {fastai.__version__}\")\n",
    "print(f\"pytorch: {torch.__version__}\")\n",
    "print(f\"numpy: {np.__version__}\")\n",
    "print(f\"cupy: {cp.__version__}\")\n",
    "print(f\"pandas: {pd.__version__}\")\n",
    "print(f\"matplotlib: {matplotlib.__version__}\")\n",
    "print(f\"opencv: {cv2.__version__}\")\n",
    "print(f\"torchvision: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistic(args: tuple, new_spacing: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extracting the regularized radius information of one case\n",
    "    \"\"\"\n",
    "    assert isinstance(args, tuple) and len(args) == 2\n",
    "    case_path, label = args\n",
    "    assert isinstance(label, int) and label in (0, 1)\n",
    "    nii_img_path = case_path / f\"{case_path.name}.nii\"\n",
    "    assert isinstance(case_path, Path) and case_path.exists()\n",
    "    assert nii_img_path.exists()\n",
    "    assert isinstance(new_spacing, np.ndarray) and new_spacing.ndim == 1 and new_spacing.size == 3\n",
    "\n",
    "    nii_img = nib.load(str(nii_img_path))\n",
    "\n",
    "    img_arr = nii_img.get_fdata()\n",
    "    assert img_arr.ndim == 3\n",
    "    assert -32768 < img_arr.min() < img_arr.max() < 32767\n",
    "    img_arr = img_arr.astype(np.int16)  # HWD\n",
    "\n",
    "    affine = nii_img.header.get_best_affine()\n",
    "\n",
    "    spacing = np.concatenate([np.abs(affine[0, :3][np.nonzero(affine[0, :3])]), np.abs(affine[1, :3][np.nonzero(affine[1, :3])]), np.abs(affine[2, :3][np.nonzero(affine[2, :3])])])  # WHD\n",
    "    spacing = spacing[::-1]  # DHW\n",
    "\n",
    "    annotation_file = case_path / \"R.acsv\"\n",
    "    assert isinstance(annotation_file, Path) and annotation_file.exists()\n",
    "\n",
    "    center_and_length = []\n",
    "    for line in open(annotation_file):\n",
    "        if line.startswith(\"point\"):\n",
    "            center_and_length.append(np.array(line.split(\"|\")[1:4]).astype(np.float))  # WHD\n",
    "    center, length = center_and_length\n",
    "\n",
    "    # Convert 'center' to pixel coordinate system\n",
    "    center = center - affine[:3, -1]\n",
    "    center = np.matmul(np.linalg.inv(affine[:3, :3]), center)  # HWD\n",
    "\n",
    "    # Convert 'length' to pixel coordinate system\n",
    "    length = np.matmul(np.linalg.inv(affine[:3, :3]), length)\n",
    "    length = np.abs(length)\n",
    "\n",
    "    new_shape = np.round(img_arr.transpose(2, 1, 0).shape * spacing / new_spacing)\n",
    "    new_spacing = spacing * img_arr.shape / new_shape\n",
    "    resize_factor = new_shape / img_arr.transpose(2, 1, 0).shape  # DHW\n",
    "    length *= resize_factor[2]\n",
    "\n",
    "    return spacing.min(), length.max(), label\n",
    "\n",
    "\n",
    "if args.pre_processing and args.test_code:\n",
    "    min_spacing, max_length, label = get_statistic(args=(Path(\"/Projects/data/DJJ/part1/410759\"), 1), new_spacing=np.array(args.spacing))\n",
    "    print(f\"min_spacing = {min_spacing}, max_length = {max_length}, label = {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_one_dataset(dataset_path: Path, excel_path: Path, title: str, bins: int = 50):\n",
    "    \"\"\"\n",
    "    Plotting some information about one data set\n",
    "    \"\"\"\n",
    "    assert isinstance(dataset_path, Path) and dataset_path.exists()\n",
    "    assert isinstance(excel_path, Path) and excel_path.exists()\n",
    "    assert isinstance(title, str) and title in (\"PART1\", \"TEST1\", \"TEST2\")\n",
    "    assert isinstance(bins, int) and bins > 0\n",
    "\n",
    "    label_pd = pd.read_excel(excel_path)[[\"住院号\", \"N2\"]]\n",
    "    label_pd = label_pd.set_index(\"住院号\")\n",
    "    label_dict = label_pd.to_dict()[\"N2\"]\n",
    "\n",
    "    label_dict_norm = dict()\n",
    "    for k, v in label_dict.items():\n",
    "        label_dict_norm[str(k)] = int(v)\n",
    "    l = [(dataset_path / i, label_dict_norm[i.name]) for i in dataset_path.iterdir() if i.is_dir() and i.name in label_dict_norm.keys() and i.name not in args.blacklist]\n",
    "    p = Pool(32)\n",
    "    p_get_statistic = partial(get_statistic, new_spacing=np.array(args.spacing))\n",
    "    rslt = p.map(p_get_statistic, l)\n",
    "    rslt_arr = np.array(rslt)\n",
    "\n",
    "    plt.figure(figsize=(4, 4), dpi=args.dpi)\n",
    "    plt.hist(rslt_arr[:, 0], bins=bins, alpha=0.7, density=True)  # edgecolor=\"black\",  facecolor=\"blue\",\n",
    "    plt.xlabel(\"spacing (in mm)\")\n",
    "    plt.ylabel(\"freq\")\n",
    "    plt.title(f\"{title} Spacing Min Value\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(4, 4), dpi=args.dpi)\n",
    "    plt.hist([rslt_arr[rslt_arr[:, 2] == 0, 1], rslt_arr[rslt_arr[:, 2] == 1, 1]], label=[\"class 0\", \"class1\"], bins=bins, density=True)\n",
    "    plt.xlabel(\"length (in pixels)\")\n",
    "    plt.ylabel(\"freq\")\n",
    "    plt.title(f\"{title} Nodule Length Max Value\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing (if Needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T07:43:34.785440Z",
     "start_time": "2020-11-22T07:43:34.755238Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_scan_nii(case_path: Path) -> [np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load the numpy matrix and spacing information from a patient's nii file\n",
    "    \"\"\"\n",
    "    assert isinstance(case_path, Path) and case_path.exists()\n",
    "\n",
    "    nii_img_path = case_path / f\"{case_path.name}.nii\"\n",
    "    assert nii_img_path.exists()\n",
    "\n",
    "    nii_img = nib.load(str(nii_img_path))\n",
    "    header = nii_img.header\n",
    "\n",
    "    img_arr = nii_img.get_fdata()\n",
    "    img_arr = np.squeeze(img_arr).astype(np.int16)\n",
    "\n",
    "    affine = header.get_best_affine()\n",
    "\n",
    "    assert isinstance(img_arr, np.ndarray)\n",
    "    assert img_arr.ndim == 3 and img_arr.dtype == np.int16\n",
    "    assert isinstance(affine, np.ndarray)\n",
    "    assert affine.ndim == 2 and affine.dtype == np.float64\n",
    "    return img_arr, affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T07:43:34.850341Z",
     "start_time": "2020-11-22T07:43:34.816267Z"
    }
   },
   "outputs": [],
   "source": [
    "def re_sample(ct_array: np.ndarray, x: [int, float], y: [int, float], z: [int, float], r_mm: [int, float], spacing: np.ndarray, new_spacing: np.ndarray, order: int = 3) -> tuple:\n",
    "    \"\"\"\n",
    "    Use the new_spacing parameter to resample the initial CT matrix with spatial resolution, while \n",
    "    mapping the coordinates, size, and other parameters to the pixel coordinate system at the \n",
    "    new resolution.\n",
    "    \"\"\"\n",
    "    assert isinstance(ct_array, np.ndarray) and ct_array.ndim == 3 and ct_array.shape > (1, 1, 1) and ct_array.dtype == np.int16\n",
    "    assert isinstance(x, (int, float)) and x >= 0\n",
    "    assert isinstance(y, (int, float)) and y >= 0\n",
    "    assert isinstance(z, (int, float)) and z >= 0\n",
    "    assert (isinstance(r_mm, (int, float)) and r_mm > 0) or r_mm is None\n",
    "    assert isinstance(spacing, np.ndarray) and spacing.size == 3\n",
    "    assert isinstance(new_spacing, np.ndarray) and new_spacing.size == 3\n",
    "    assert isinstance(order, int) and 0 <= order <= 5\n",
    "    assert (spacing > 0).all() and (new_spacing > 0).all()\n",
    "\n",
    "    new_shape = np.round(ct_array.shape * spacing / new_spacing)\n",
    "    new_spacing = spacing * ct_array.shape / new_shape\n",
    "    resize_factor = new_shape / ct_array.shape\n",
    "    ct_array = zoom(ct_array, resize_factor, mode=\"nearest\", order=order)\n",
    "    z *= resize_factor[0]\n",
    "    y *= resize_factor[1]\n",
    "    x *= resize_factor[2]\n",
    "    if r_mm is not None:\n",
    "        r_mm *= resize_factor[2]\n",
    "\n",
    "    assert isinstance(ct_array, np.ndarray) and ct_array.ndim == 3\n",
    "    assert ct_array.shape > (1, 1, 1) and ct_array.dtype == np.int16\n",
    "    assert isinstance(new_spacing, np.ndarray) and new_spacing.size == 3\n",
    "    assert isinstance(x, (int, float)) and x >= 0\n",
    "    assert isinstance(y, (int, float)) and y >= 0\n",
    "    assert isinstance(z, (int, float)) and z >= 0\n",
    "    assert (isinstance(r_mm, (int, float)) and r_mm > 0) or r_mm is None\n",
    "\n",
    "    return ct_array, new_spacing, x, y, z, r_mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodule(ct_array: np.ndarray, x: [int, float], y: [int, float], z: [int, float], radius_mm: [int, float] = None, fixed_radius: int = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    The 3D-patch is cropped from the CT matrix of a case based on the nodule annotation.\n",
    "    \"\"\"\n",
    "    assert isinstance(ct_array, np.ndarray) and ct_array.ndim == 3 and ct_array.dtype == np.int16 and ct_array.shape > (1, 1, 1)\n",
    "    assert isinstance(x, (int, float)) and x >= 0\n",
    "    assert isinstance(y, (int, float)) and y >= 0\n",
    "    assert isinstance(z, (int, float)) and z >= 0\n",
    "\n",
    "    if fixed_radius is None:\n",
    "        assert isinstance(radius_mm, (int, float)) and radius_mm > 0\n",
    "    if radius_mm is None:\n",
    "        assert isinstance(fixed_radius, int) and fixed_radius > 0\n",
    "\n",
    "    x, y, z = int(x), int(y), int(z)\n",
    "    if fixed_radius is not None and radius_mm is None:\n",
    "        ct_array = np.pad(ct_array, pad_width=fixed_radius, mode=\"constant\", constant_values=ct_array.min())\n",
    "        ct_array = ct_array[z : z + fixed_radius * 2, y : y + fixed_radius * 2, x : x + fixed_radius * 2]  # DHW\n",
    "        assert ct_array.shape == (fixed_radius * 2, fixed_radius * 2, fixed_radius * 2), \"abnormal nodule size\" + ct_array.shape\n",
    "    elif radius_mm is not None and fixed_radius is None:\n",
    "        radius_mm = int(radius_mm)\n",
    "        ct_array = np.pad(ct_array, pad_width=radius_mm, mode=\"constant\", constant_values=ct_array.min())\n",
    "        ct_array = ct_array[\n",
    "            z : z + radius_mm * 2, y : y + radius_mm * 2, x : x + radius_mm * 2,\n",
    "        ]\n",
    "        assert ct_array.shape == (radius_mm * 2, radius_mm * 2, radius_mm * 2), \"abnormal nodule size\"\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    assert ct_array.dtype == np.int16\n",
    "    return ct_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T07:43:34.881145Z",
     "start_time": "2020-11-22T07:43:34.851760Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_3d_roi(case_path: Path, diameter_size: int, new_spacing: np.ndarray, fixed_radius: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract 3D-ROI.\n",
    "    \"\"\"\n",
    "    nii_img_path = case_path / f\"{case_path.name}.nii\"\n",
    "\n",
    "    assert isinstance(case_path, Path) and case_path.exists()\n",
    "    assert nii_img_path.exists()\n",
    "\n",
    "    assert (isinstance(diameter_size, int) and diameter_size > 0 and diameter_size % 2 == 0) or diameter_size is None\n",
    "    assert isinstance(new_spacing, np.ndarray)\n",
    "    assert isinstance(fixed_radius, int) and fixed_radius > 0\n",
    "    assert new_spacing.ndim == 1 and new_spacing.size == 3\n",
    "\n",
    "    img_arr, affine = load_scan_nii(case_path=case_path)\n",
    "\n",
    "    old_spacing = np.concatenate([np.abs(affine[0, :3][np.nonzero(affine[0, :3])]), np.abs(affine[1, :3][np.nonzero(affine[1, :3])]), np.abs(affine[2, :3][np.nonzero(affine[2, :3])]),])  # WHD\n",
    "\n",
    "    annotation_file = case_path / \"R.acsv\"\n",
    "    assert isinstance(annotation_file, Path) and annotation_file.exists()\n",
    "\n",
    "    center_and_length = []\n",
    "    for line in open(annotation_file):\n",
    "        if line.startswith(\"point\"):\n",
    "            center_and_length.append(np.array(line.split(\"|\")[1:4]).astype(np.float))  # WHD\n",
    "\n",
    "    center, length = center_and_length\n",
    "\n",
    "    center = center - affine[:3, -1]\n",
    "    center = np.matmul(np.linalg.inv(affine[:3, :3]), center)\n",
    "\n",
    "    length = np.matmul(np.linalg.inv(affine[:3, :3]), length)\n",
    "    length = np.abs(length)\n",
    "\n",
    "    img_arr, new_spacing, x, y, z, r_mm = re_sample(\n",
    "        ct_array=img_arr.transpose(2, 1, 0), x=center[0], y=center[1], z=center[2], r_mm=diameter_size // 2 if diameter_size is not None else None, spacing=old_spacing[::-1], new_spacing=new_spacing,\n",
    "    )\n",
    "\n",
    "    patch_3d = get_nodule(ct_array=img_arr, x=x, y=y, z=z, radius_mm=None, fixed_radius=fixed_radius)\n",
    "\n",
    "    assert patch_3d.shape == (fixed_radius * 2, fixed_radius * 2, fixed_radius * 2)\n",
    "    assert patch_3d.dtype == np.int16\n",
    "    return patch_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_case_path_and_label(ct_root_path: Path, label_info_lst: list) -> list:\n",
    "    \"\"\"\n",
    "    Check data integrity.\n",
    "    \"\"\"\n",
    "    assert isinstance(ct_root_path, Path) and ct_root_path.exists() and ct_root_path.is_dir()\n",
    "    assert isinstance(label_info_lst, list) and len(label_info_lst) > 0\n",
    "\n",
    "    cases_path_lst = [ct_root_path / sub_dir for sub_dir in ct_root_path.iterdir() if sub_dir.is_dir()]\n",
    "\n",
    "    r = []\n",
    "    for label in label_info_lst:\n",
    "        found = False\n",
    "        case_path = None\n",
    "        for case_path in cases_path_lst:\n",
    "            if label[\"ID\"] == case_path.stem:\n",
    "                found = True\n",
    "                break\n",
    "        if found:\n",
    "            label[\"path\"] = case_path\n",
    "            r.append(label)\n",
    "        else:\n",
    "            print(f'Corresponding folder not found: case {label[\"ID\"]}')\n",
    "            pass\n",
    "    assert len(r) > 0\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T07:43:34.948281Z",
     "start_time": "2020-11-22T07:43:34.916620Z"
    }
   },
   "outputs": [],
   "source": [
    "def worker_for_one_case(para: dict, processed_path: Path, category_names: list, fixed_radius: int, spacing: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Worker function that extracts node from a patient and saves the .npy file to the hard drive\n",
    "    \"\"\"\n",
    "    assert isinstance(para, dict)\n",
    "    assert isinstance(processed_path, Path) and processed_path.is_dir()\n",
    "    assert isinstance(category_names, list)\n",
    "    assert isinstance(category_names[0], str)\n",
    "    assert isinstance(fixed_radius, int) and fixed_radius > 0\n",
    "    assert isinstance(spacing, np.ndarray) and spacing.size == 3 and (spacing > 0).all()\n",
    "\n",
    "    npy_folder = processed_path / category_names[para[\"c\"]]\n",
    "    npy_file = npy_folder / f'{para[\"ID\"]}.npy'\n",
    "\n",
    "    if not npy_folder.exists():\n",
    "        npy_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if args.use_existing and npy_file.exists():\n",
    "        print(f\"{npy_file} file existed, skip\")\n",
    "    else:\n",
    "        try:\n",
    "            patch_3d = get_3d_roi(case_path=para[\"path\"], diameter_size=None, new_spacing=spacing, fixed_radius=fixed_radius)\n",
    "            assert patch_3d.dtype == np.int16\n",
    "            np.save(str(npy_file), patch_3d)\n",
    "        except IOError:\n",
    "            print(f\"Bug in {npy_file}\")\n",
    "        else:\n",
    "            print(f\"file {npy_file} saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def final_pre_processing(ct_root: Path, excel_f: Path, dataset_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Process all patient data from the dataset into *.npy files.\n",
    "    \"\"\"\n",
    "\n",
    "    assert isinstance(ct_root, Path) and ct_root.exists() and ct_root.is_dir()\n",
    "    assert isinstance(excel_f, Path) and excel_f.exists() and excel_f.is_file()\n",
    "    assert isinstance(dataset_name, str) and dataset_name in (\"part1\", \"test1\", \"test2\")\n",
    "\n",
    "    label_pd = pd.read_excel(excel_f)[[\"住院号\", \"N2\"]]\n",
    "    label_pd = label_pd.set_index(\"住院号\")\n",
    "\n",
    "    label_lst = [{\"ID\": str(index), \"c\": int(item[\"N2\"]),} for index, item in label_pd.iterrows() if str(index) not in args.blacklist]\n",
    "    case_path_label_list = get_case_path_and_label(label_info_lst=label_lst, ct_root_path=ct_root)\n",
    "\n",
    "    output_path = args.output_path / ct_root.stem\n",
    "\n",
    "    if not output_path.exists():\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    partial_worker = partial(worker_for_one_case, processed_path=output_path, category_names=[\"negative\", \"positive\"], fixed_radius=args.fixed_half_edge, spacing=np.array(args.spacing))\n",
    "\n",
    "    p = Pool(cpu_count())\n",
    "    print(f\"Start processing {dataset_name} in parallel.\")\n",
    "    p.map(partial_worker, case_path_label_list)\n",
    "    print(f\"dataset {dataset_name} is processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract 3D Nodule Patches from All Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T07:43:35.084179Z",
     "start_time": "2020-11-22T07:43:35.058343Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if args.pre_processing:\n",
    "    final_pre_processing(ct_root=args.ct_path_p1, excel_f=args.excel_f_p1, dataset_name=\"part1\")\n",
    "    final_pre_processing(ct_root=args.ct_path_t1, excel_f=args.excel_f_t1, dataset_name=\"test1\")\n",
    "    final_pre_processing(ct_root=args.ct_path_t2, excel_f=args.excel_f_t2, dataset_name=\"test2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_dataset(path: Path, r: tuple, seed: int) -> None:\n",
    "    \"\"\"\n",
    "    Shuffle the *.npy files\n",
    "    \"\"\"\n",
    "    assert isinstance(path, Path) and path.exists() and path.is_dir()\n",
    "    assert isinstance(r, tuple) and len(r) == 2 and 0 < r[0] < r[1] < 1\n",
    "    assert isinstance(seed, int)\n",
    "\n",
    "    categories = [\"negative\", \"positive\"]\n",
    "    original = [[], []]\n",
    "    for child in path.glob(\"**/**/*\"):\n",
    "        if child.is_file() and child.suffix == \".npy\":\n",
    "            if \"negative\" in str(child):\n",
    "                original[0].append(child)\n",
    "            elif \"positive\" in str(child):\n",
    "                original[1].append(child)\n",
    "            else:\n",
    "                raise ValueError\n",
    "\n",
    "    print(len(original[0]))\n",
    "    print(len(original[1]))\n",
    "    original[0] = sorted(original[0], key=lambda x: x.stem)\n",
    "    original[1] = sorted(original[1], key=lambda x: x.stem)\n",
    "\n",
    "    random.seed(seed)\n",
    "    random.shuffle(original[0])\n",
    "    random.shuffle(original[1])\n",
    "\n",
    "    for cate_i in range(2):\n",
    "        for i, npy_f in enumerate(original[cate_i]):\n",
    "            if 0 <= i < int(r[0] * len(original[cate_i])):  # train\n",
    "                npy_f_new = path / \"train\" / categories[cate_i] / npy_f.name\n",
    "            elif int(r[0] * len(original[cate_i])) <= i < int(r[1] * len(original[cate_i])):  # valid\n",
    "                npy_f_new = path / \"valid\" / categories[cate_i] / npy_f.name\n",
    "            elif int(r[1] * len(original[cate_i])) <= i < len(original[cate_i]):  # test\n",
    "                npy_f_new = path / \"test\" / categories[cate_i] / npy_f.name\n",
    "            else:\n",
    "                raise ValueError\n",
    "            if not npy_f_new.parent.exists():\n",
    "                npy_f_new.parent.mkdir(parents=True, exist_ok=True)\n",
    "            print(npy_f, \"=>\", npy_f_new)\n",
    "            shutil.move(npy_f, npy_f_new)\n",
    "\n",
    "\n",
    "if args.shuffle_dataset:\n",
    "    shuffle_dataset(path=args.output_path / \"part1\", r=args.split_p, seed=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Extracted 3D RoI in Test 0 DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T07:43:56.694344Z",
     "start_time": "2020-11-22T07:43:35.085542Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_and_check_test_set(test_set_path: Path, k=100):\n",
    "\n",
    "    assert isinstance(test_set_path, Path) and test_set_path.exists() and test_set_path.is_dir()\n",
    "    assert isinstance(k, int) and k > 0\n",
    "\n",
    "    plt.figure(figsize=(15, 15), dpi=args.dpi)\n",
    "    plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "\n",
    "    count = 1\n",
    "    for npy_file in random.sample(list(test_set_path.glob(\"**/*.npy\")), k=k):\n",
    "        file_name, c = npy_file.stem, npy_file.parent.stem\n",
    "\n",
    "        x = np.load(str(npy_file))  # .astype(np.int16)\n",
    "        x = np.clip(x, a_min=args.wl - args.ww / 2, a_max=args.wl + args.ww / 2)\n",
    "        x = (x - (args.wl - args.ww / 2)) / args.ww\n",
    "        x = x[x.shape[0] // 2]\n",
    "\n",
    "        plt.subplot(10, 10, count)\n",
    "        plt.imshow(x, \"gray\")\n",
    "        plt.title(file_name + \"/\" + str(c), fontsize=10)\n",
    "        count += 1\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if args.plot_validate_result:\n",
    "    plot_and_check_test_set(test_set_path=args.output_path / \"part1\" / \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Extracted 3D RoI in Test 1 DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T07:43:56.694344Z",
     "start_time": "2020-11-22T07:43:35.085542Z"
    }
   },
   "outputs": [],
   "source": [
    "if args.plot_validate_result:\n",
    "    plot_and_check_test_set(test_set_path=args.output_path / \"test1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Extracted 3D RoI in Test 2 DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T07:43:56.694344Z",
     "start_time": "2020-11-22T07:43:35.085542Z"
    }
   },
   "outputs": [],
   "source": [
    "if args.plot_validate_result:\n",
    "    plot_and_check_test_set(test_set_path=args.output_path / \"test2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Fastai Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CaseImage (Customized Class for Loading CT Case Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T09:04:55.739182Z",
     "start_time": "2020-09-09T09:04:54.139508Z"
    }
   },
   "outputs": [],
   "source": [
    "def random_rotation(volume, rotation, length):\n",
    "    \"\"\"\n",
    "    3D Affine Transform\n",
    "    \"\"\"\n",
    "    assert isinstance(volume, np.ndarray) and volume.ndim == 3\n",
    "    assert isinstance(rotation, int) and rotation > 0\n",
    "\n",
    "    theta_x = np.pi / 180 * np.random.uniform(-rotation, rotation)\n",
    "    theta_y = np.pi / 180 * np.random.uniform(-rotation, rotation)\n",
    "    theta_z = np.pi / 180 * np.random.uniform(-rotation, rotation)\n",
    "\n",
    "    rotation_matrix_x = np.array([[1, 0, 0], [0, np.cos(theta_x), -np.sin(theta_x)], [0, np.sin(theta_x), np.cos(theta_x)],])\n",
    "    rotation_matrix_y = np.array([[np.cos(theta_y), 0, np.sin(theta_y)], [0, 1, 0], [-np.sin(theta_y), 0, np.cos(theta_y)],])\n",
    "    rotation_matrix_z = np.array([[np.cos(theta_z), -np.sin(theta_z), 0], [np.sin(theta_z), np.cos(theta_z), 0], [0, 0, 1],])\n",
    "    transform_matrix = rotation_matrix_x @ rotation_matrix_y @ rotation_matrix_z\n",
    "\n",
    "    center_in = 0.5 * np.array(volume.shape)\n",
    "    center_out = 0.5 * np.array(volume.shape)\n",
    "\n",
    "    if cp.cuda.is_available():  # affine transform on GPU\n",
    "        volume_rotated = cu_ndimage.affine_transform(\n",
    "            input=cp.asarray(volume), matrix=cp.asarray(transform_matrix), offset=cp.asarray(center_in - center_out.dot(transform_matrix.T)), order=1, mode=\"nearest\", cval=0\n",
    "        )\n",
    "        volume_rotated = volume_rotated.get()\n",
    "    else:  # affine transform on CPU\n",
    "        volume_rotated = affine_transform(volume, transform_matrix, mode=\"constant\", cval=0, offset=center_in - center_out.dot(transform_matrix.T), order=3)\n",
    "\n",
    "    assert isinstance(volume_rotated, np.ndarray) and volume_rotated.dtype == np.int16\n",
    "    return volume_rotated\n",
    "\n",
    "\n",
    "class CaseImage(fastuple):\n",
    "    def show(self, ctx=None, **kwargs):\n",
    "        img, target = self\n",
    "        assert isinstance(img, (PIL.Image.Image, Tensor))\n",
    "        assert target is None or isinstance(target, (str, int)), f\"target data type：{type(target)}\"\n",
    "\n",
    "        if not isinstance(img, Tensor):\n",
    "            img = tensor(img).permute(2, 0, 1)\n",
    "\n",
    "        img = img.byte()\n",
    "\n",
    "        return show_image(torch.cat([img[0], img[1], img[2],], dim=1,), cmap=\"gray\", figsize=(5, 15), title=target, ctx=ctx, **kwargs,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CaseTransform (Customized Transform for Loading CT Case Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T09:04:55.739182Z",
     "start_time": "2020-09-09T09:04:54.139508Z"
    }
   },
   "outputs": [],
   "source": [
    "class CaseTransform(Transform):\n",
    "    \"\"\"\n",
    "    Input a npy path and output a CaseImage object\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, files, label_func, splits):\n",
    "        assert isinstance(files, L)\n",
    "        assert hasattr(label_func, \"__call__\")\n",
    "        assert isinstance(splits, tuple)\n",
    "\n",
    "        self.vocab, self.label2id = uniqueify(list(map(label_func, files)), sort=True, bidir=True,)\n",
    "        self.label_func = label_func\n",
    "        self.train_lst = [f for f in files[splits[0]]]\n",
    "\n",
    "    def encodes(self, fn):\n",
    "        assert isinstance(fn, Path) and fn.is_file(), f\"Got {type(fn)}\"\n",
    "\n",
    "        f = np.load(str(fn)).astype(np.int16)\n",
    "\n",
    "        _, h, w = f.shape\n",
    "        i_middle = f.shape[0] // 2\n",
    "\n",
    "        if fn in self.train_lst:\n",
    "            f = random_rotation(volume=f, rotation=args.rotation, length=h)\n",
    "        f = np.stack([f[i_middle, :, :], f[:, i_middle, :], f[:, :, i_middle],], axis=2)\n",
    "\n",
    "        assert f.shape == (h, w, 3)\n",
    "\n",
    "        f = np.uint8(np.clip((f - (args.wl - args.ww / 2)) / args.ww * 255, a_min=0, a_max=255))\n",
    "        assert f.dtype == np.uint8\n",
    "\n",
    "        cls = self.label2id[self.label_func(fn)]\n",
    "        return CaseImage(PILImage.create(f), cls)\n",
    "\n",
    "    def decodes(self, x: PILImage) -> matplotlib.axes.SubplotBase:\n",
    "        assert isinstance(x, PILImage), f\"Wrong data type: {type(x)}\"\n",
    "\n",
    "        if not isinstance(x, Tensor):\n",
    "            img = tensor(x).permute(2, 0, 1)\n",
    "\n",
    "        img = img.byte()\n",
    "        return show_image(torch.cat([img[0], img[1], img[2],], dim=1), cmap=\"gray\", figsize=(5, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customized Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T09:04:55.743675Z",
     "start_time": "2020-09-09T09:04:55.740158Z"
    }
   },
   "outputs": [],
   "source": [
    "@typedispatch\n",
    "def show_batch(x: CaseImage, y, samples, ctxs=None, max_n=6, nrows=None, ncols=2, figsize=None, **kwargs):\n",
    "    assert isinstance(x, CaseImage)\n",
    "    assert y is None\n",
    "    assert ctxs is None\n",
    "    assert samples is None\n",
    "    assert isinstance(max_n, int) and max_n > 0\n",
    "    assert nrows is None or isinstance(nrows, int) and nrows > 0\n",
    "    assert isinstance(ncols, int) and ncols > 0\n",
    "    assert figsize is None or isinstance(figsize, tuple)\n",
    "\n",
    "    if figsize is None:\n",
    "        figsize = (ncols * 4, max_n // ncols * 2)\n",
    "    if ctxs is None:\n",
    "        ctxs = get_grid(min(x[0].shape[0], max_n), nrows=None, ncols=ncols, figsize=figsize)\n",
    "    for i, ctx in enumerate(ctxs):\n",
    "        CaseImage(x[0][i], x[1][i].item()).show(ctx=ctx)\n",
    "\n",
    "\n",
    "def get_balance_files(files: L, train_name: str, valid_name: str) -> L:\n",
    "    \"\"\"\n",
    "    Down-sampling negative class during training.\n",
    "    \"\"\"\n",
    "    assert isinstance(files, L)\n",
    "    assert isinstance(train_name, str)\n",
    "    assert isinstance(valid_name, str)\n",
    "\n",
    "    new_files = L()\n",
    "    tmp_train_negative = L()\n",
    "    positive_count = 0\n",
    "    for i in files.items:\n",
    "        if i.parent.parent.stem == train_name:\n",
    "            if i.parent.stem == \"positive\":\n",
    "                new_files.append(i)\n",
    "                positive_count += 1\n",
    "            elif i.parent.stem == \"negative\":\n",
    "                tmp_train_negative.append(i)\n",
    "            else:\n",
    "                raise ValueError\n",
    "        else:\n",
    "            new_files.append(i)\n",
    "    random.shuffle(tmp_train_negative)\n",
    "    for i in tmp_train_negative[: int(positive_count / args.r_pos)]:\n",
    "        new_files.append(i)\n",
    "    return new_files\n",
    "\n",
    "\n",
    "class Sharpen(RandTransform):\n",
    "    \"\"\"\n",
    "    Sharpen image using opencv.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def encodes(self, x: PIL.Image.Image):\n",
    "        x = cv2.filter2D(np.array(x), -1, self.kernel)\n",
    "\n",
    "        x = PIL.Image.fromarray(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Smoothing(RandTransform):\n",
    "    \"\"\"\n",
    "    Add Gaussian Smoothing\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size=2, denominator=9, **kwargs):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.denominator = denominator\n",
    "        self.kernel = np.array([[1, 2, 1], [2, 4, 2], [1, 2, 1]]) / self.denominator\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def encodes(self, x: PIL.Image.Image):\n",
    "        x = cv2.filter2D(np.array(x), -1, self.kernel)\n",
    "\n",
    "        x = PIL.Image.fromarray(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_dls(path, train_name, valid_name, verbose=True, balance=True):\n",
    "    \"\"\"\n",
    "    Get Balanced DataLoaders\n",
    "    \"\"\"\n",
    "    assert isinstance(path, Path) and path.is_dir()\n",
    "    assert isinstance(train_name, str)\n",
    "    assert isinstance(valid_name, str)\n",
    "    assert isinstance(balance, bool)\n",
    "\n",
    "    files = get_files(path=path, extensions=\".npy\", recurse=True,)\n",
    "\n",
    "    if balance:\n",
    "        files = get_balance_files(files=files, train_name=train_name, valid_name=valid_name)\n",
    "\n",
    "    splits = GrandparentSplitter(train_name=train_name, valid_name=valid_name,)(files)\n",
    "    tfm = CaseTransform(files=files, label_func=parent_label, splits=splits)\n",
    "    tls = TfmdLists(files, tfms=tfm, splits=splits, verbose=verbose)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Building Dataloaders\")\n",
    "    dataloaders = tls.dataloaders(\n",
    "        after_item=[\n",
    "            RandomCrop(size=args.radius_middle * 2),\n",
    "            RandomResizedCrop(size=args.resize_size, min_scale=0.4, ratio=(0.75, 1.3333333333333333), resamples=(2, 0),),\n",
    "            ToTensor,\n",
    "            Sharpen(p=0.5),\n",
    "            Smoothing(kernel_size=2, denominator=16, p=0.5),\n",
    "        ],\n",
    "        after_batch=[IntToFloatTensor, Normalize.from_stats(*imagenet_stats)],\n",
    "        verbose=verbose,\n",
    "        bs=args.batch_size,\n",
    "        num_workers=args.num_workers,\n",
    "    )\n",
    "\n",
    "    return dataloaders\n",
    "\n",
    "\n",
    "if args.test_code:\n",
    "    dls = get_dls(path=args.output_path / \"part1\", train_name=\"train\", valid_name=\"valid\")\n",
    "    len(dls.train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Training and Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T09:04:55.743675Z",
     "start_time": "2020-09-09T09:04:55.740158Z"
    }
   },
   "outputs": [],
   "source": [
    "dls = get_dls(path=args.output_path / \"part1\", train_name=\"train\", valid_name=\"valid\")\n",
    "if args.test_code:\n",
    "    f\"----------Plot 25 Cases in Training Dataset----------\"\n",
    "    dls.show_batch(max_n=25, nrows=5, ncols=5, figsize=(30, 10), dpi=args.dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T09:05:08.964763Z",
     "start_time": "2020-09-09T09:05:08.423806Z"
    }
   },
   "outputs": [],
   "source": [
    "if args.test_code:\n",
    "    f\"----------Plot 25 Cases in Validation Dataset----------\"\n",
    "    dls.valid.show_batch(max_n=25, nrows=5, ncols=5, figsize=(30, 10), dpi=args.dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Test0 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T09:05:10.133936Z",
     "start_time": "2020-09-09T09:05:08.965638Z"
    }
   },
   "outputs": [],
   "source": [
    "test0_files = get_files(path=args.output_path / \"part1\" / \"test\", extensions=\".npy\", recurse=True,)\n",
    "dl_t0 = dls.test_dl(test0_files, shuffle=False)\n",
    "if args.test_code:\n",
    "    dl_t0.show_batch(max_n=25, nrows=5, ncols=5, figsize=(30, 10), dpi=args.dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Test1 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T09:05:11.348521Z",
     "start_time": "2020-09-09T09:05:10.135082Z"
    }
   },
   "outputs": [],
   "source": [
    "test1_files = get_files(path=args.output_path / \"test1\", extensions=\".npy\", recurse=True,)\n",
    "dl_t1 = dls.test_dl(test1_files, shuffle=False)\n",
    "if args.test_code:\n",
    "    dl_t1.show_batch(max_n=25, nrows=5, ncols=5, figsize=(30, 10), dpi=args.dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Test2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T09:05:12.466546Z",
     "start_time": "2020-09-09T09:05:11.349415Z"
    }
   },
   "outputs": [],
   "source": [
    "test2_files = get_files(path=args.output_path / \"test2\", extensions=\".npy\", recurse=True,)\n",
    "dl_t2 = dls.test_dl(test2_files, shuffle=False)\n",
    "if args.test_code:\n",
    "    dl_t2.show_batch(max_n=25, nrows=5, ncols=5, figsize=(30, 10), dpi=args.dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T09:05:12.470986Z",
     "start_time": "2020-09-09T09:05:12.467424Z"
    }
   },
   "outputs": [],
   "source": [
    "def sensitivity(y_pred, y_true, thresh=0.5):\n",
    "    \"\"\"\n",
    "    sensitivity metric function.\n",
    "    \"\"\"\n",
    "    y_pred = F.softmax(y_pred, dim=1)[:, 1]\n",
    "    TP = ((y_pred > thresh) * (y_true.data)).float().sum()\n",
    "    # 预测为negative实际为positive的总数\n",
    "    FN = ((y_pred <= thresh) * (y_true.data)).float().sum()\n",
    "    return (TP / (TP + FN)).item()\n",
    "\n",
    "\n",
    "def specificity(y_pred, y_true, thresh=0.5):\n",
    "    \"\"\"\n",
    "    specificity metric function.\n",
    "    \"\"\"\n",
    "    y_pred = F.softmax(y_pred, dim=1)[:, 1]\n",
    "    FP = ((y_pred > thresh).float() * (1 - y_true.data).float()).sum()\n",
    "    TN = ((y_pred <= thresh).float() * (1 - y_true.data).float()).sum()\n",
    "    return (TN / (FP + TN)).item()\n",
    "\n",
    "\n",
    "def Precision(axis=-1, labels=None, pos_label=1, average=\"binary\", sample_weight=None):\n",
    "    \"\"\"\n",
    "    Precision for single-label classification\n",
    "    \"\"\"\n",
    "    return skm_to_fastai(skm.precision_score, axis=axis, labels=labels, pos_label=pos_label, average=average, sample_weight=sample_weight, zero_division=0)  # 对原API的补充\n",
    "\n",
    "\n",
    "def Recall(axis=-1, labels=None, pos_label=1, average=\"binary\", sample_weight=None):\n",
    "    \"\"\"\n",
    "    Recall for single-label classification\n",
    "    \"\"\"\n",
    "    return skm_to_fastai(skm.recall_score, axis=axis, labels=labels, pos_label=pos_label, average=average, sample_weight=sample_weight, zero_division=0)  # 对原API的补充"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T09:05:12.571102Z",
     "start_time": "2020-09-09T09:05:12.472096Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyNN(Module):\n",
    "    def __init__(self, encoder, head):\n",
    "        self.encoder, self.head = encoder, head\n",
    "\n",
    "    def forward(self, x):\n",
    "        ftrs = self.encoder(x)\n",
    "        return self.head(ftrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T09:05:13.326827Z",
     "start_time": "2020-09-09T09:05:12.575821Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder = create_body(arch=resnet152, n_in=args.n_in, pretrained=True, cut=-2)\n",
    "encoder[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T09:05:13.339533Z",
     "start_time": "2020-09-09T09:05:13.327639Z"
    }
   },
   "outputs": [],
   "source": [
    "head = create_head(512 * 4 * 2, n_out=2, ps=args.ps)\n",
    "head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T09:05:13.450170Z",
     "start_time": "2020-09-09T09:05:13.341478Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = MyNN(encoder, head)\n",
    "model\n",
    "\n",
    "\n",
    "def my_nn_splitter(model):\n",
    "    return [params(model.encoder), params(model.head)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T09:05:13.539694Z",
     "start_time": "2020-09-09T09:05:13.458893Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_func = CrossEntropyLossFlat(weight=torch.tensor(args.loss_weight).cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_func = partial(Adam, mom=args.mom, sqr_mom=0.999, wd=args.wd, decouple_wd=False, eps=1e-08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance Sample Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBalanceCallback(Callback):\n",
    "    def before_train(self):\n",
    "        self.learn.dls = get_dls(path=args.output_path / \"part1\", train_name=\"train\", valid_name=\"valid\", verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T09:05:24.151909Z",
     "start_time": "2020-09-09T09:05:13.544409Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn = Learner(\n",
    "    dls=dls,\n",
    "    model=model,\n",
    "    loss_func=loss_func,\n",
    "    opt_func=opt_func,\n",
    "    splitter=my_nn_splitter,\n",
    "    path=Path(\".\"),\n",
    "    model_dir=Path(\"weights\"),\n",
    "    metrics=[RocAucBinary(), accuracy, Recall(), Precision()],\n",
    "    wd_bn_bias=False,\n",
    "    cbs=[MixUp(), DataBalanceCallback()],\n",
    ")\n",
    "\n",
    "learn.summary()\n",
    "learn.show_training_loop()\n",
    "print(learn.cbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training （head fine-tuning + full network finetuning）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T09:09:02.834183Z",
     "start_time": "2020-09-09T09:05:24.152740Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.freeze()\n",
    "learn.summary()\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T10:07:32.580127Z",
     "start_time": "2020-09-09T09:09:02.835501Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.fit(\n",
    "    n_epoch=args.freeze_epochs, lr=args.lr, wd=args.wd, cbs=[SaveModelCallback(monitor=\"roc_auc_score\", fname=f\"bestmodel-stage1-{args.exp_name}\", with_opt=False)],\n",
    ")\n",
    "\n",
    "learn.load(file=f\"bestmodel-stage1-{args.exp_name}\")\n",
    "learn.unfreeze()\n",
    "learn.fit(\n",
    "    n_epoch=args.epoch, lr=args.lr / 10, wd=args.wd, cbs=[SaveModelCallback(monitor=\"roc_auc_score\", fname=f\"bestmodel-stage2-{args.exp_name}\", with_opt=False)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T10:07:32.679800Z",
     "start_time": "2020-09-09T10:07:32.581110Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.recorder.plot_loss(with_valid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Results on the Five Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results on Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T07:52:36.622642Z",
     "start_time": "2020-11-22T07:52:36.543180Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_top_losses(x, y, samples, outs, raws, losses, nrows=None, ncols=None, figsize=None, **kwargs):\n",
    "    axs = get_grid(len(samples), nrows=nrows, ncols=ncols, add_vert=1, figsize=figsize, title=\"Prediction/Actual/Loss/Probability\", dpi=args.dpi)\n",
    "    for ax, s, o, r, l in zip(axs, samples, outs, raws, losses):\n",
    "        s[0].show(ctx=ax, **kwargs)\n",
    "        ax.set_title(f\"{o[0].argmax(-1)}/{s[1]} / {l.item():.2f} / {r.max().item():.2f}\")\n",
    "\n",
    "\n",
    "class MyInterpretation(Interpretation):\n",
    "    \"Interpretation base class, can be inherited for task specific Interpretation classes\"\n",
    "\n",
    "    def __init__(self, dl, inputs, preds, targs, decoded, losses):\n",
    "        super().__init__(dl, inputs, preds, targs, decoded, losses)\n",
    "\n",
    "    def _pre_show_batch(self, b, max_n=9):\n",
    "        \"Decode `b` to be ready for `show_batch`\"\n",
    "        b = self.dl.decode(b)\n",
    "        its = L()\n",
    "        for x1, y1 in zip(b[0], b[1]):\n",
    "            its.append((CaseImage(x1, None), y1))\n",
    "        if not is_listy(b):\n",
    "            b, its = [b], L((o,) for o in its)\n",
    "        return detuplify(b[: self.dl.n_inp]), detuplify(b[self.dl.n_inp :]), its\n",
    "\n",
    "    @classmethod\n",
    "    def from_learner(cls, learn, ds_idx=-1, dl=None, act=None):\n",
    "        \"Construct interpretation object from a learner\"\n",
    "        if ds_idx in (0, 1):\n",
    "            return cls(dl, *learn.get_preds(ds_idx=ds_idx, with_input=True, with_loss=True, with_decoded=True, act=act))\n",
    "        else:\n",
    "            return cls(dl, *learn.get_preds(dl=dl, with_input=True, with_loss=True, with_decoded=True, act=act))\n",
    "\n",
    "    def plot_top_losses(self, k, largest=True, **kwargs):\n",
    "        losses, idx = self.top_losses(k, largest)\n",
    "        if not isinstance(self.inputs, tuple):\n",
    "            self.inputs = (self.inputs,)\n",
    "\n",
    "        if isinstance(self.inputs[0], Tensor):\n",
    "            inps = tuple(o[idx] for o in self.inputs)\n",
    "        else:\n",
    "            inps = self.dl.create_batch(self.dl.before_batch([tuple(o[i] for o in self.inputs) for i in idx]))\n",
    "\n",
    "        b = inps + tuple(o[idx] for o in (self.targs if is_listy(self.targs) else (self.targs,)))\n",
    "\n",
    "        x, y, its = self._pre_show_batch(b, max_n=k)\n",
    "\n",
    "        b_out = inps + tuple(o[idx] for o in (self.decoded if is_listy(self.decoded) else (self.decoded,)))\n",
    "\n",
    "        x1, y1, outs = self._pre_show_batch(b_out, max_n=k)\n",
    "        if its is not None:\n",
    "            plot_top_losses(x, y, its, outs.itemgot(slice(len(inps), None)), self.preds[idx], losses, **kwargs)\n",
    "\n",
    "\n",
    "class MyClassificationInterpretation(MyInterpretation):\n",
    "    \"Interpretation methods for classification models.\"\n",
    "\n",
    "    def __init__(self, dl, inputs, preds, targs, decoded, losses):\n",
    "        super().__init__(dl, inputs, preds, targs, decoded, losses)\n",
    "        self.vocab = self.dl.vocab\n",
    "\n",
    "    def confusion_matrix(self):\n",
    "        \"Confusion matrix as an `np.ndarray`.\"\n",
    "        x = torch.arange(0, len(self.vocab))\n",
    "        d, t = flatten_check(self.decoded.argmax(-1), self.targs)\n",
    "        cm = ((d == x[:, None]) & (t == x[:, None, None])).long().sum(2)\n",
    "        return to_np(cm)\n",
    "\n",
    "    def plot_confusion_matrix(self, normalize=False, title=\"Confusion matrix\", cmap=\"Blues\", norm_dec=2, plot_txt=True, **kwargs):\n",
    "        \"Plot the confusion matrix, with `title` and using `cmap`.\"\n",
    "        cm = self.confusion_matrix()\n",
    "        if normalize:\n",
    "            cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "        fig = plt.figure(**kwargs)\n",
    "        plt.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n",
    "        plt.title(title)\n",
    "        tick_marks = np.arange(len(self.vocab))\n",
    "        plt.xticks(tick_marks, self.vocab, rotation=90)\n",
    "        plt.yticks(tick_marks, self.vocab, rotation=0)\n",
    "\n",
    "        if plot_txt:\n",
    "            thresh = cm.max() / 2.0\n",
    "            for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "                coeff = f\"{cm[i, j]:.{norm_dec}f}\" if normalize else f\"{cm[i, j]}\"\n",
    "                plt.text(j, i, coeff, horizontalalignment=\"center\", verticalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "        ax = fig.gca()\n",
    "        ax.set_ylim(len(self.vocab) - 0.5, -0.5)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.grid(False)\n",
    "\n",
    "    def most_confused(self, min_val=1):\n",
    "        \"Sorted descending list of largest non-diagonal entries of confusion matrix, presented as actual, predicted, number of occurrences.\"\n",
    "        cm = self.confusion_matrix()\n",
    "        np.fill_diagonal(cm, 0)\n",
    "        res = [(self.vocab[i], self.vocab[j], cm[i, j]) for i, j in zip(*np.where(cm >= min_val))]\n",
    "        return sorted(res, key=itemgetter(2), reverse=True)\n",
    "\n",
    "    def print_classification_report(self):\n",
    "        \"Print scikit-learn classification report\"\n",
    "        d, t = flatten_check(self.decoded, self.targs)\n",
    "        print(skm.classification_report(t, d, labels=list(self.vocab.o2i.values()), target_names=[str(v) for v in self.vocab]))\n",
    "\n",
    "\n",
    "def validate_one_set(learn, dl=None, ds_idx=-1, dataset_name=None):\n",
    "    assert isinstance(dataset_name, str)\n",
    "    res = learn.validate(ds_idx=ds_idx, dl=dl,)\n",
    "\n",
    "    print(f\"{dataset_name}: loss = {res[0]}, auc = {res[1]}, accuracy = {res[2]}, sensitivity(recall) = {res[3]}, precision = {res[4]}\")\n",
    "    interp = MyClassificationInterpretation.from_learner(learn=learn, dl=dl, ds_idx=ds_idx, act=partial(F.softmax, dim=-1))\n",
    "    interp.plot_confusion_matrix(title=f\"Confusion Matrix of {dataset_name} Set\", dpi=args.dpi)\n",
    "    interp.plot_top_losses(k=25, largest=True, figsize=(30, 10))\n",
    "\n",
    "\n",
    "class Hook:\n",
    "    def __init__(self, m):\n",
    "        self.hook = m.register_forward_hook(self.hook_func)\n",
    "\n",
    "    def hook_func(self, m, i, o):\n",
    "        self.stored = o.detach().clone()\n",
    "\n",
    "    def __enter__(self, *args):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.hook.remove()\n",
    "\n",
    "\n",
    "class HookBwd:\n",
    "    def __init__(self, m):\n",
    "        self.hook = m.register_backward_hook(self.hook_func)\n",
    "\n",
    "    def hook_func(self, m, gi, go):\n",
    "        self.stored = go[0].detach().clone()\n",
    "\n",
    "    def __enter__(self, *args):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.hook.remove()\n",
    "\n",
    "\n",
    "def generate_hotmap(cls, x_idx, layer_idx, ax, x, y, x_dec):\n",
    "    assert isinstance(cls, int) and cls in (0, 1)\n",
    "    assert isinstance(x_idx, int) and x_idx >= 0\n",
    "    assert isinstance(layer_idx, int)\n",
    "    assert isinstance(x, Tensor)\n",
    "    assert isinstance(y, Tensor)\n",
    "    assert isinstance(x_dec, Tensor)\n",
    "\n",
    "    with HookBwd(learn.model.encoder[layer_idx]) as hookg:\n",
    "        with Hook(learn.model.encoder[layer_idx]) as hook:\n",
    "            output = learn.model.eval()(x.cuda())\n",
    "            print(\"prediction: \", F.softmax(output, dim=-1))\n",
    "            act = hook.stored\n",
    "        output[0, cls].backward()\n",
    "        grad = hookg.stored\n",
    "\n",
    "    w = grad[0].mean(dim=[1, 2], keepdim=True)\n",
    "    cam_map = (w * act[0]).sum(0)\n",
    "\n",
    "    cam_map = cam_map.detach().cpu()\n",
    "    cam_map = tensor(cv2.resize(cam_map.numpy(), dsize=x_dec.shape[-2:], interpolation=cv2.INTER_CUBIC))\n",
    "    im = ax.imshow(torch.cat([cam_map, cam_map, cam_map], dim=1), alpha=0.5, cmap=\"jet\", vmin=args.vmin, vmax=args.vmax)\n",
    "\n",
    "    return im\n",
    "\n",
    "\n",
    "def interpret_one_case(dl, layer_idx, sample_index, title):\n",
    "    assert isinstance(dl, fastai.data.core.TfmdDL)\n",
    "    assert isinstance(layer_idx, int)\n",
    "    assert isinstance(sample_index, int)\n",
    "    assert isinstance(title, str)\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=3, ncols=1, figsize=(30, 10), dpi=args.dpi)\n",
    "\n",
    "    xs, ys = first(dl)\n",
    "    x = xs[sample_index][None]\n",
    "    y = ys[sample_index]\n",
    "    print(\"target =\", y)\n",
    "    x_dec = dl.decode((x,))[0][0]\n",
    "    case_image = CaseImage(x_dec, None)\n",
    "\n",
    "    for i, ax in enumerate(axs.reshape(-1)):\n",
    "        if i == 0:\n",
    "            case_image.show(ctx=ax)\n",
    "        elif i == 1:\n",
    "            case_image.show(ctx=ax)\n",
    "            im = generate_hotmap(cls=0, x_idx=sample_index, layer_idx=layer_idx, ax=ax, x=x, y=y, x_dec=x_dec)\n",
    "        elif i == 2:\n",
    "            case_image.show(ctx=ax)\n",
    "            im = generate_hotmap(cls=1, x_idx=sample_index, layer_idx=layer_idx, ax=ax, x=x, y=y, x_dec=x_dec)\n",
    "\n",
    "    cbar_ax = fig.add_axes([0.65, 0.15, 0.025, 0.7])\n",
    "    cbar = fig.colorbar(im, cax=cbar_ax,)\n",
    "\n",
    "    cbar.ax.tick_params(labelsize=0, axis=\"both\", which=\"both\", length=0)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T07:53:04.750309Z",
     "start_time": "2020-11-22T07:52:36.623876Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.load(file=f\"bestmodel-stage2-{args.exp_name}\")\n",
    "learn.dls = get_dls(path=args.output_path / \"part1\", train_name=\"train\", valid_name=\"valid\", verbose=False, balance=False)\n",
    "\n",
    "if args.plot_validate_result:\n",
    "    validate_one_set(learn=learn, dl=learn.dls.train, ds_idx=0, dataset_name=\"Train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T07:53:25.443992Z",
     "start_time": "2020-11-22T07:53:04.751756Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if args.plot_validate_result:\n",
    "    validate_one_set(learn=learn, dl=learn.dls.valid, ds_idx=1, dataset_name=\"Validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results on Test0 Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T07:54:07.845338Z",
     "start_time": "2020-11-22T07:53:25.445643Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if args.plot_validate_result:\n",
    "    validate_one_set(learn=learn, dl=dl_t0, dataset_name=\"Test0\")\n",
    "\n",
    "    random.seed(8)\n",
    "    sample_index = random.choice(range(min(args.batch_size, len(dl_t0.items))))\n",
    "    interpret_one_case(dl=dl_t0, layer_idx=-1, sample_index=sample_index, title=\"Test0\")\n",
    "    random.seed(24)\n",
    "    sample_index = random.choice(range(min(args.batch_size, len(dl_t0.items))))\n",
    "    interpret_one_case(dl=dl_t0, layer_idx=-1, sample_index=sample_index, title=\"Test0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results on Test1 Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T07:54:07.845338Z",
     "start_time": "2020-11-22T07:53:25.445643Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if args.plot_validate_result:\n",
    "    validate_one_set(learn=learn, dl=dl_t1, dataset_name=\"Test1\")\n",
    "\n",
    "    random.seed(8)\n",
    "    sample_index = random.choice(range(min(args.batch_size, len(dl_t1.items))))\n",
    "    interpret_one_case(dl=dl_t1, layer_idx=-1, sample_index=sample_index, title=\"Test1\")\n",
    "    random.seed(24)\n",
    "    sample_index = random.choice(range(min(args.batch_size, len(dl_t1.items))))\n",
    "    interpret_one_case(dl=dl_t1, layer_idx=-1, sample_index=sample_index, title=\"Test1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results on Test2 Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T07:54:07.845338Z",
     "start_time": "2020-11-22T07:53:25.445643Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if args.plot_validate_result:\n",
    "    validate_one_set(learn=learn, dl=dl_t2, dataset_name=\"Test2\")\n",
    "\n",
    "    random.seed(8)\n",
    "    sample_index = random.choice(range(min(args.batch_size, len(dl_t2.items))))\n",
    "    interpret_one_case(dl=dl_t2, layer_idx=-1, sample_index=sample_index, title=\"Test2\")\n",
    "    random.seed(24)\n",
    "    sample_index = random.choice(range(min(args.batch_size, len(dl_t2.items))))\n",
    "    interpret_one_case(dl=dl_t2, layer_idx=-1, sample_index=sample_index, title=\"Test2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Prediction to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_prediction_excel(ids, pred, targs, excel_path):\n",
    "    \"\"\"\n",
    "    Output prediction results of train, valid and test dataset to a excel file.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(np.concatenate((ids[:, np.newaxis], pred, targs[:, np.newaxis]), axis=-1))\n",
    "    df.to_excel(excel_path, index=False)\n",
    "\n",
    "\n",
    "if not (args.output_path / args.exp_name).exists():\n",
    "    (args.output_path / args.exp_name).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# train set\n",
    "preds, targets = learn.get_preds(ds_idx=0, reorder=True)\n",
    "ids = np.array([str(i).split(\"/\")[-1].split(\".\")[0] for i in learn.dls.train.items])\n",
    "\n",
    "save_prediction_excel(ids, F.softmax(preds, dim=-1).cpu().numpy(), targets, args.output_path / args.exp_name / \"predict_train.xlsx\")\n",
    "\n",
    "# validation set\n",
    "preds, targets = learn.get_preds(ds_idx=1)\n",
    "ids = np.array([str(i).split(\"/\")[-1].split(\".\")[0] for i in learn.dls.valid.items])\n",
    "\n",
    "save_prediction_excel(ids, F.softmax(preds, dim=-1).cpu().numpy(), targets, args.output_path / args.exp_name / \"predict_valid.xlsx\")\n",
    "\n",
    "# test0 set\n",
    "preds, targets = learn.get_preds(dl=dl_t0)\n",
    "ids = np.array([str(i).split(\"/\")[-1].split(\".\")[0] for i in dl_t0.items])\n",
    "\n",
    "save_prediction_excel(ids, F.softmax(preds, dim=-1).cpu().numpy(), targets, args.output_path / args.exp_name / \"predict_test0.xlsx\")\n",
    "\n",
    "# test1 set\n",
    "preds, targets = learn.get_preds(dl=dl_t1)\n",
    "ids = np.array([str(i).split(\"/\")[-1].split(\".\")[0] for i in dl_t1.items])\n",
    "\n",
    "save_prediction_excel(ids, F.softmax(preds, dim=-1).cpu().numpy(), targets, args.output_path / args.exp_name / \"predict_test1.xlsx\")\n",
    "\n",
    "# test2 set\n",
    "preds, targets = learn.get_preds(dl=dl_t2)\n",
    "ids = np.array([str(i).split(\"/\")[-1].split(\".\")[0] for i in dl_t2.items])\n",
    "\n",
    "save_prediction_excel(ids, F.softmax(preds, dim=-1).cpu().numpy(), targets, args.output_path / args.exp_name / \"predict_test2.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T13:11:31.647541Z",
     "start_time": "2020-11-15T13:11:26.108666Z"
    }
   },
   "outputs": [],
   "source": [
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "# Train Set\n",
    "preds, targets = learn.get_preds(ds_idx=0)\n",
    "fpr[0], tpr[0], _ = skm.roc_curve(y_true=targets, y_score=F.softmax(preds, dim=-1).cpu().numpy()[:, 1])\n",
    "roc_auc[0] = skm.auc(fpr[0], tpr[0])\n",
    "\n",
    "# Valid Set\n",
    "preds, targets = learn.get_preds(ds_idx=1)\n",
    "fpr[1], tpr[1], _ = skm.roc_curve(y_true=targets, y_score=F.softmax(preds, dim=-1).cpu().numpy()[:, 1])\n",
    "roc_auc[1] = skm.auc(fpr[1], tpr[1])\n",
    "\n",
    "# Test0 Set\n",
    "preds, targets = learn.get_preds(dl=dl_t0)\n",
    "fpr[2], tpr[2], _ = skm.roc_curve(y_true=targets, y_score=F.softmax(preds, dim=-1).cpu().numpy()[:, 1])\n",
    "roc_auc[2] = skm.auc(fpr[2], tpr[2])\n",
    "\n",
    "# Test1 Set\n",
    "preds, targets = learn.get_preds(dl=dl_t1)\n",
    "fpr[3], tpr[3], _ = skm.roc_curve(y_true=targets, y_score=F.softmax(preds, dim=-1).cpu().numpy()[:, 1])\n",
    "roc_auc[3] = skm.auc(fpr[3], tpr[3])\n",
    "\n",
    "# Test2 Set\n",
    "preds, targets = learn.get_preds(dl=dl_t2)\n",
    "fpr[4], tpr[4], _ = skm.roc_curve(y_true=targets, y_score=F.softmax(preds, dim=-1).cpu().numpy()[:, 1])\n",
    "roc_auc[4] = skm.auc(fpr[4], tpr[4])\n",
    "\n",
    "plt.figure(dpi=args.dpi, figsize=(5, 5))\n",
    "lw = 2\n",
    "plt.plot(fpr[0], tpr[0], color=\"blue\", lw=lw, label=\"Train Set ROC curve (AUC = %0.2f)\" % roc_auc[0])\n",
    "plt.plot(fpr[1], tpr[1], color=\"green\", lw=lw, label=\"Validation Set ROC curve (AUC = %0.2f)\" % roc_auc[1])\n",
    "plt.plot(fpr[2], tpr[2], color=\"darkorange\", lw=lw, label=\"Test0 Set ROC curve (AUC = %0.2f)\" % roc_auc[2])\n",
    "plt.plot(fpr[3], tpr[3], color=\"yellow\", lw=lw, label=\"Test1 Set ROC curve (AUC = %0.2f)\" % roc_auc[3])\n",
    "plt.plot(fpr[4], tpr[4], color=\"red\", lw=lw, label=\"Test2 Set ROC curve (AUC = %0.2f)\" % roc_auc[4])\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\n",
    "    \"1 - Specificity\", fontsize=11,\n",
    ")\n",
    "plt.ylabel(\n",
    "    \"Sensitivity\", fontsize=11,\n",
    ")\n",
    "plt.title(\n",
    "    \"Receiver Operating Characteristic\", fontsize=11,\n",
    ")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}